{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  2   2   4   4   0 241]\n",
      "  [  2   2   4   5   0 241]\n",
      "  [  2   2   4   4   0 241]\n",
      "  ...\n",
      "  [234 234  15  17   0 241]\n",
      "  [232 230  21  26   0 241]\n",
      "  [235 229  50  36   0 241]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path='best-int8 (2).tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load and preprocess the image.\n",
    "image_path = 'img1.jpeg'\n",
    "input_shape = input_details[0]['shape']\n",
    "image = Image.open(image_path).resize((input_shape[1], input_shape[2]))\n",
    "image_array = np.array(image)\n",
    "image_array = image_array.astype(np.uint8)  # Convert image array to UINT8\n",
    "input_data = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Set the input tensor to the image data.\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run the inference.\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor and convert it to a numpy array.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Print the result.\n",
    "print(output_data)\n",
    "\n",
    "# Convert the output array to PIL Image\n",
    "output_image = Image.fromarray(np.squeeze(output_data))\n",
    "\n",
    "# Save the image to a file\n",
    "output_image.save('output_image.JPG')\n",
    "\n",
    "# Display the image\n",
    "output_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path='best-int8 (2).tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load and preprocess the image.\n",
    "image_path = 'img4.jpg'\n",
    "\n",
    "input_shape = input_details[0]['shape'][1:3]  # Excluding batch dimension\n",
    "image = Image.open(image_path).resize(input_shape)\n",
    "image_array = np.array(image) / 255.0  # Normalize pixel values between 0 and 1\n",
    "input_data = np.expand_dims(image_array, axis=0).astype(input_details[0]['dtype'])\n",
    "\n",
    "# Set the input tensor to the image data.\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run the inference.\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor and convert it to a numpy array.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Convert the output tensor to an image.\n",
    "output_image = Image.fromarray(np.squeeze(output_data).astype(np.uint8))\n",
    "\n",
    "# Print the image.\n",
    "output_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "generic_type: type \"InterpreterWrapper\" is already registered!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtflite_runtime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpreter\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtflite\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Load the PyTorch model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mbest.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tflite_runtime\\interpreter.py:36\u001b[0m\n\u001b[0;32m     33\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_export \u001b[39mas\u001b[39;00m _tf_export\n\u001b[0;32m     34\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m   \u001b[39m# This file is part of tflite_runtime package.\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtflite_runtime\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_tensorflow_interpreter_wrapper \u001b[39mas\u001b[39;00m _interpreter_wrapper\n\u001b[0;32m     38\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39m_tf_export\u001b[39m(\u001b[39m*\u001b[39mx, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     39\u001b[0m     \u001b[39mdel\u001b[39;00m x, kwargs\n",
      "\u001b[1;31mImportError\u001b[0m: generic_type: type \"InterpreterWrapper\" is already registered!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "# Load the PyTorch model\n",
    "model = torch.load('best.pt')\n",
    "model.eval()\n",
    "\n",
    "# Preprocess the input image\n",
    "image_path = 'img4.jpg'\n",
    "image = Image.open(image_path)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "# Convert the PyTorch model to TensorFlow Lite\n",
    "dummy_input = torch.randn(1, 3, 640, 640)\n",
    "torch.onnx.export(model, dummy_input, 'model.onnx', verbose=True)\n",
    "converter = tflite.TFLiteConverter.from_onnx('model.onnx')\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Load the TensorFlow Lite model\n",
    "interpreter = tflite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Set the input tensor to the preprocessed image\n",
    "input_tensor = np.array(input_batch, dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor and post-process the results\n",
    "output_tensor = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_classes = np.argmax(output_tensor, axis=1)\n",
    "\n",
    "# Print the predicted class\n",
    "class_labels = ['scartch/Damage']  # Replace with your own class labels\n",
    "predicted_class = class_labels[output_classes[0]]\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Anonymous Guy/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-6-1 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m input_tensor \u001b[39m=\u001b[39m preprocess(image)\n\u001b[0;32m     20\u001b[0m \u001b[39m# Perform object detection\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m results \u001b[39m=\u001b[39m model(input_tensor)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Extract bounding box coordinates, confidences, and class labels\u001b[39;00m\n\u001b[0;32m     24\u001b[0m boxes \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mxyxy[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:676\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[1;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ims, torch\u001b[39m.\u001b[39mTensor):  \u001b[39m# torch\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast(autocast):\n\u001b[1;32m--> 676\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(ims\u001b[39m.\u001b[39;49mto(p\u001b[39m.\u001b[39;49mdevice)\u001b[39m.\u001b[39;49mtype_as(p), augment\u001b[39m=\u001b[39;49maugment)  \u001b[39m# inference\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39m# Pre-process\u001b[39;00m\n\u001b[0;32m    679\u001b[0m n, ims \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(ims), \u001b[39mlist\u001b[39m(ims)) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ims, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39melse\u001b[39;00m (\u001b[39m1\u001b[39m, [ims])  \u001b[39m# number, list of images\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:508\u001b[0m, in \u001b[0;36mDetectMultiBackend.forward\u001b[1;34m(self, im, augment, visualize)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, im, augment\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, visualize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    507\u001b[0m     \u001b[39m# YOLOv5 MultiBackend inference\u001b[39;00m\n\u001b[1;32m--> 508\u001b[0m     b, ch, h, w \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mshape  \u001b[39m# batch, channel, height, width\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mand\u001b[39;00m im\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mfloat16:\n\u001b[0;32m    510\u001b[0m         im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mhalf()  \u001b[39m# to FP16\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='best.pt')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess the input image\n",
    "image_path = '00.jpg'\n",
    "image = Image.open(image_path)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(image)\n",
    "\n",
    "# Perform object detection\n",
    "results = model(input_tensor)\n",
    "\n",
    "# Extract bounding box coordinates, confidences, and class labels\n",
    "boxes = results.xyxy[0].tolist()\n",
    "confidences = results.xyxy[0, :, 4].tolist()\n",
    "class_ids = results.xyxy[0, :, 5].tolist()\n",
    "labels = results.names[0]\n",
    "\n",
    "# Display the detected objects\n",
    "for box, confidence, class_id in zip(boxes, confidences, class_ids):\n",
    "    x1, y1, x2, y2 = box\n",
    "    label = labels[int(class_id)]\n",
    "    print('Object:', label, 'Confidence:', confidence)\n",
    "    print('Bounding Box:', (x1, y1), (x2, y2))\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbest.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      7\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(model_path)  \u001b[39m# Use map_location to load on CPU if trained on GPU\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[39m=\u001b[39m checkpoint[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[39m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv5 model from a local folder\n",
    "model_path = 'best.pt'\n",
    "model = torch.load(model_path)  # Use map_location to load on CPU if trained on GPU\n",
    "model = checkpoint['model']\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load the image\n",
    "image_path = \"image.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Preprocess the image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = preprocess(image).unsqueeze(0)\n",
    "\n",
    "# Detect objects\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# Extract bounding box coordinates, confidences, and class labels\n",
    "boxes = output.xyxy[0].tolist()\n",
    "confidences = output.xyxy[0, :, 4].tolist()\n",
    "class_ids = output.xyxy[0, :, 5].tolist()\n",
    "labels = output.names[0]\n",
    "\n",
    "# Draw bounding boxes on the image\n",
    "for box, confidence, class_id in zip(boxes, confidences, class_ids):\n",
    "    x1, y1, x2, y2 = box\n",
    "    label = labels[int(class_id)]\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "    cv2.putText(image, label, (x1, y1 + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow(\"Image with detected objects\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!streamlit hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run detector_made.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.system(\"python detect.py --weights \\\"best.pt\\\" --img 640 --conf 0.25 --source \\\"image_find.jpg\\\" > output.txt 2>&1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['best.pt'], source=image_find.jpg, data=data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "fatal: cannot change to 'C:\\Users\\Anonymous': Invalid argument\n",
      "YOLOv5  2023-5-28 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers...\n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "image 1/1 C:\\Users\\Anonymous Guy\\Desktop\\yolov5\\image_find.jpg: 640x640 (no detections), 14.0ms\n",
      "Speed: 1.0ms pre-process, 14.0ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\exp4\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"output.txt\"\n",
    "import re\n",
    "# Read the content of the file\n",
    "with open(filename, \"r\") as file:\n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        # Print each line\n",
    "        print(line.strip())\n",
    "filename = \"output.txt\"\n",
    "extracted_string=\"\"\n",
    "# Read the content of the file\n",
    "with open(filename, \"r\") as file:\n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        match = re.search(r'runs\\\\detect\\\\(exp\\d+)\\.', line)\n",
    "        if match:\n",
    "              extracted_string = match.group(1)\n",
    "        # Print each line\n",
    "        #print(line.strip())\n",
    "print(extracted_string)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
